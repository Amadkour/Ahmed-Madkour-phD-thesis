% Generated by IEEEtran.bst, version: 1.12 (2007/01/11)
\begin{thebibliography}{100}
\providecommand{\url}[1]{#1}
\csname url@samestyle\endcsname
\providecommand{\newblock}{\relax}
\providecommand{\bibinfo}[2]{#2}
\providecommand{\BIBentrySTDinterwordspacing}{\spaceskip=0pt\relax}
\providecommand{\BIBentryALTinterwordstretchfactor}{4}
\providecommand{\BIBentryALTinterwordspacing}{\spaceskip=\fontdimen2\font plus
\BIBentryALTinterwordstretchfactor\fontdimen3\font minus \fontdimen4\font\relax}
\providecommand{\BIBforeignlanguage}[2]{{%
\expandafter\ifx\csname l@#1\endcsname\relax
\typeout{** WARNING: IEEEtran.bst: No hyphenation pattern has been}%
\typeout{** loaded for the language `#1'. Using the pattern for}%
\typeout{** the default language instead.}%
\else
\language=\csname l@#1\endcsname
\fi
#2}}
\providecommand{\BIBdecl}{\relax}
\BIBdecl

\bibitem{yang2021concept}
C.~Yang, Y.-m. Cheung, J.~Ding, and K.~C. Tan, ``Concept drift-tolerant transfer learning in dynamic environments,'' \emph{IEEE Transactions on Neural Networks and Learning Systems}, vol.~33, no.~8, pp. 3857--3871, 2021.

\bibitem{dong2019multistream}
B.~Dong, Y.~Gao, S.~Chandra, and L.~Khan, ``Multistream classification with relative density ratio estimation,'' in \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, vol.~33, no.~01, 2019, pp. 3478--3485.

\bibitem{shan2018online}
J.~Shan, H.~Zhang, W.~Liu, and Q.~Liu, ``Online active learning ensemble framework for drifted data streams,'' \emph{IEEE transactions on neural networks and learning systems}, vol.~30, no.~2, pp. 486--498, 2018.

\bibitem{pan2009survey}
S.~J. Pan and Q.~Yang, ``A survey on transfer learning,'' \emph{IEEE Transactions on knowledge and data engineering}, vol.~22, no.~10, pp. 1345--1359, 2009.

\bibitem{zhuang2020comprehensive}
F.~Zhuang, Z.~Qi, K.~Duan, D.~Xi, Y.~Zhu, H.~Zhu, H.~Xiong, and Q.~He, ``A comprehensive survey on transfer learning,'' \emph{Proceedings of the IEEE}, vol. 109, no.~1, pp. 43--76, 2020.

\bibitem{wang2018systematic}
S.~Wang, L.~L. Minku, and X.~Yao, ``A systematic study of online class imbalance learning with concept drift,'' \emph{IEEE transactions on neural networks and learning systems}, vol.~29, no.~10, pp. 4802--4821, 2018.

\bibitem{sun2009classification}
Y.~Sun, A.~K. Wong, and M.~S. Kamel, ``Classification of imbalanced data: A review,'' \emph{International journal of pattern recognition and artificial intelligence}, vol.~23, no.~04, pp. 687--719, 2009.

\bibitem{charte2015addressing}
F.~Charte, A.~J. Rivera, M.~J. del Jesus, and F.~Herrera, ``Addressing imbalance in multilabel classification: Measures and random resampling algorithms,'' \emph{Neurocomputing}, vol. 163, pp. 3--16, 2015.

\bibitem{charte2015mlsmote}
------, ``Mlsmote: Approaching imbalanced multilabel learning through synthetic instance generation,'' \emph{Knowledge-Based Systems}, vol.~89, pp. 385--397, 2015.

\bibitem{daniels2017addressing}
Z.~Daniels and D.~Metaxas, ``Addressing imbalance in multi-label classification using structured hellinger forests,'' in \emph{Proceedings of the AAAI conference on artificial intelligence}, vol.~31, no.~1, 2017.

\bibitem{liu2018making}
B.~Liu and G.~Tsoumakas, ``Making classifier chains resilient to class imbalance,'' in \emph{Asian Conference on Machine Learning}.\hskip 1em plus 0.5em minus 0.4em\relax PMLR, 2018, pp. 280--295.

\bibitem{bhowan2012evolving}
U.~Bhowan, M.~Johnston, M.~Zhang, and X.~Yao, ``Evolving diverse ensembles using genetic programming for classification with unbalanced data,'' \emph{IEEE Transactions on Evolutionary Computation}, vol.~17, no.~3, pp. 368--386, 2012.

\bibitem{galar2011review}
M.~Galar, A.~Fernandez, E.~Barrenechea, H.~Bustince, and F.~Herrera, ``A review on ensembles for the class imbalance problem: bagging-, boosting-, and hybrid-based approaches,'' \emph{IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)}, vol.~42, no.~4, pp. 463--484, 2011.

\bibitem{cruz2018dynamic}
R.~M. Cruz, R.~Sabourin, and G.~D. Cavalcanti, ``Dynamic classifier selection: Recent advances and perspectives,'' \emph{Information Fusion}, vol.~41, pp. 195--216, 2018.

\bibitem{kuncheva2000clustering}
L.~I. Kuncheva, ``Clustering-and-selection model for classifier combination,'' in \emph{KES'2000. Fourth International Conference on Knowledge-Based Intelligent Engineering Systems and Allied Technologies. Proceedings (Cat. No. 00TH8516)}, vol.~1.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2000, pp. 185--188.

\bibitem{woloszynski2011probabilistic}
T.~Woloszynski and M.~Kurzynski, ``A probabilistic model of classifier competence for dynamic ensemble selection,'' \emph{Pattern Recognition}, vol.~44, no. 10-11, pp. 2656--2668, 2011.

\bibitem{lysiak2014optimal}
R.~Lysiak, M.~Kurzynski, and T.~Woloszynski, ``Optimal selection of ensemble classifiers using measures of competence and diversity of base classifiers,'' \emph{Neurocomputing}, vol. 126, pp. 29--35, 2014.

\bibitem{cruz2017meta}
R.~M. Cruz, R.~Sabourin, and G.~D. Cavalcanti, ``Meta-des. oracle: Meta-learning and feature selection for dynamic ensemble selection,'' \emph{Information fusion}, vol.~38, pp. 84--103, 2017.

\bibitem{chawla2003smoteboost}
N.~V. Chawla, A.~Lazarevic, L.~O. Hall, and K.~W. Bowyer, ``Smoteboost: Improving prediction of the minority class in boosting,'' in \emph{Knowledge Discovery in Databases: PKDD 2003: 7th European Conference on Principles and Practice of Knowledge Discovery in Databases, Cavtat-Dubrovnik, Croatia, September 22-26, 2003. Proceedings 7}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2003, pp. 107--119.

\bibitem{wang2010negative}
S.~Wang, H.~Chen, and X.~Yao, ``Negative correlation learning for classification ensembles,'' in \emph{The 2010 international joint conference on neural networks (IJCNN)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2010, pp. 1--8.

\bibitem{widmer1996learning}
G.~Widmer and M.~Kubat, ``Learning in the presence of concept drift and hidden contexts,'' \emph{Machine learning}, vol.~23, pp. 69--101, 1996.

\bibitem{lu2016concept}
N.~Lu, J.~Lu, G.~Zhang, and R.~L. De~Mantaras, ``A concept drift-tolerant case-base editing technique,'' \emph{Artificial Intelligence}, vol. 230, pp. 108--133, 2016.

\bibitem{gama2014survey}
J.~Gama, I.~{\v{Z}}liobait{\.e}, A.~Bifet, M.~Pechenizkiy, and A.~Bouchachia, ``A survey on concept drift adaptation,'' \emph{ACM computing surveys (CSUR)}, vol.~46, no.~4, pp. 1--37, 2014.

\bibitem{losing2016knn}
V.~Losing, B.~Hammer, and H.~Wersing, ``Knn classifier with self adjusting memory for heterogeneous concept drift,'' in \emph{2016 IEEE 16th international conference on data mining (ICDM)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2016, pp. 291--300.

\bibitem{storkey2008training}
A.~Storkey, ``When training and test sets are different: characterizing learning transfer,'' 2008.

\bibitem{8496795}
J.~Lu, A.~Liu, F.~Dong, F.~Gu, J.~Gama, and G.~Zhang, ``Learning under concept drift: A review,'' \emph{IEEE Transactions on Knowledge and Data Engineering}, vol.~31, no.~12, pp. 2346--2363, 2019.

\bibitem{ramirez2017survey}
S.~Ram{\'\i}rez-Gallego, B.~Krawczyk, S.~Garc{\'\i}a, M.~Wo{\'z}niak, and F.~Herrera, ``A survey on data preprocessing for data stream mining: Current status and future directions,'' \emph{Neurocomputing}, vol. 239, pp. 39--57, 2017.

\bibitem{silva2013data}
J.~A. Silva, E.~R. Faria, R.~C. Barros, E.~R. Hruschka, A.~C.~d. Carvalho, and J.~Gama, ``Data stream clustering: A survey,'' \emph{ACM Computing Surveys (CSUR)}, vol.~46, no.~1, pp. 1--31, 2013.

\bibitem{dries2009adaptive}
A.~Dries and U.~R{\"u}ckert, ``Adaptive concept drift detection,'' \emph{Statistical Analysis and Data Mining: The ASA Data Science Journal}, vol.~2, no. 5-6, pp. 311--327, 2009.

\bibitem{alippi2008just}
C.~Alippi and M.~Roveri, ``Just-in-time adaptive classifiers—part i: Detecting nonstationary changes,'' \emph{IEEE Transactions on Neural Networks}, vol.~19, no.~7, pp. 1145--1153, 2008.

\bibitem{gama2004learning}
J.~Gama, P.~Medas, G.~Castillo, and P.~Rodrigues, ``Learning with drift detection,'' in \emph{Advances in Artificial Intelligence--SBIA 2004: 17th Brazilian Symposium on Artificial Intelligence, Sao Luis, Maranhao, Brazil, September 29-Ocotber 1, 2004. Proceedings 17}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2004, pp. 286--295.

\bibitem{bu2016pdf}
L.~Bu, C.~Alippi, and D.~Zhao, ``A pdf-free change detection test based on density difference estimation,'' \emph{IEEE transactions on neural networks and learning systems}, vol.~29, no.~2, pp. 324--334, 2016.

\bibitem{venkatasubramanianinformation}
T.~D. S. K.~S. Venkatasubramanian and K.~Yi, ``An information-theoretic approach to detecting changes in multi-dimensional data streams.''

\bibitem{frias2014online}
I.~Frias-Blanco, J.~del Campo-{\'A}vila, G.~Ramos-Jimenez, R.~Morales-Bueno, A.~Ortiz-Diaz, and Y.~Caballero-Mota, ``Online and non-parametric drift detection methods based on hoeffding’s bounds,'' \emph{IEEE Transactions on Knowledge and Data Engineering}, vol.~27, no.~3, pp. 810--823, 2014.

\bibitem{yamada2013change}
M.~Yamada, A.~Kimura, F.~Naya, and H.~Sawada, ``Change-point detection with feature selection in high-dimensional time-series data,'' in \emph{Twenty-Third International Joint Conference on Artificial Intelligence}, 2013.

\bibitem{bach2008paired}
S.~H. Bach and M.~A. Maloof, ``Paired learners for concept drift,'' in \emph{2008 Eighth IEEE International Conference on Data Mining}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2008, pp. 23--32.

\bibitem{bifet2007learning}
A.~Bifet and R.~Gavalda, ``Learning from time-changing data with adaptive windowing,'' in \emph{Proceedings of the 2007 SIAM international conference on data mining}.\hskip 1em plus 0.5em minus 0.4em\relax SIAM, 2007, pp. 443--448.

\bibitem{xu2017dynamic}
S.~Xu and J.~Wang, ``Dynamic extreme learning machine for data stream classification,'' \emph{Neurocomputing}, vol. 238, pp. 433--449, 2017.

\bibitem{liu2016fp}
D.~Liu, Y.~Wu, and H.~Jiang, ``Fp-elm: An online sequential learning algorithm for dealing with concept drift,'' \emph{Neurocomputing}, vol. 207, pp. 322--334, 2016.

\bibitem{soares2016adaptive}
S.~G. Soares and R.~Ara{\'u}jo, ``An adaptive ensemble of on-line extreme learning machines with variable forgetting factor for dynamic system prediction,'' \emph{Neurocomputing}, vol. 171, pp. 693--707, 2016.

\bibitem{manly2000cumulative}
B.~F. Manly and D.~Mackenzie, ``A cumulative sum type of method for environmental monitoring,'' \emph{Environmetrics: The official journal of the International Environmetrics Society}, vol.~11, no.~2, pp. 151--166, 2000.

\bibitem{sun2018concept}
Y.~Sun, K.~Tang, Z.~Zhu, and X.~Yao, ``Concept drift adaptation by exploiting historical knowledge,'' \emph{IEEE transactions on neural networks and learning systems}, vol.~29, no.~10, pp. 4822--4832, 2018.

\bibitem{oza2001experimental}
N.~C. Oza and S.~Russell, ``Experimental comparisons of online and batch versions of bagging and boosting,'' in \emph{Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining}, 2001, pp. 359--364.

\bibitem{bifet2009new}
A.~Bifet, G.~Holmes, B.~Pfahringer, R.~Kirkby, and R.~Gavalda, ``New ensemble methods for evolving data streams,'' in \emph{Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining}, 2009, pp. 139--148.

\bibitem{chu2004fast}
F.~Chu and C.~Zaniolo, ``Fast and light boosting for adaptive mining of data streams,'' in \emph{Advances in Knowledge Discovery and Data Mining: 8th Pacific-Asia Conference, PAKDD 2004, Sydney, Australia, May 26-28, 2004. Proceedings 8}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2004, pp. 282--292.

\bibitem{gomes2017adaptive}
H.~M. Gomes, A.~Bifet, J.~Read, J.~P. Barddal, F.~Enembreck, B.~Pfharinger, G.~Holmes, and T.~Abdessalem, ``Adaptive random forests for evolving data stream classification,'' \emph{Machine Learning}, vol. 106, pp. 1469--1495, 2017.

\bibitem{kolter2007dynamic}
J.~Z. Kolter and M.~A. Maloof, ``Dynamic weighted majority: An ensemble method for drifting concepts,'' \emph{The Journal of Machine Learning Research}, vol.~8, pp. 2755--2790, 2007.

\bibitem{elwell2011incremental}
R.~Elwell and R.~Polikar, ``Incremental learning of concept drift in nonstationary environments,'' \emph{IEEE transactions on neural networks}, vol.~22, no.~10, pp. 1517--1531, 2011.

\bibitem{brzezinski2013reacting}
D.~Brzezinski and J.~Stefanowski, ``Reacting to different types of concept drift: The accuracy updated ensemble algorithm,'' \emph{IEEE transactions on neural networks and learning systems}, vol.~25, no.~1, pp. 81--94, 2013.

\bibitem{zhang2008categorizing}
P.~Zhang, X.~Zhu, and Y.~Shi, ``Categorizing and mining concept drifting data streams,'' in \emph{Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining}, 2008, pp. 812--820.

\bibitem{sun2016online}
Y.~Sun, K.~Tang, L.~L. Minku, S.~Wang, and X.~Yao, ``Online ensemble learning of data streams with gradually evolved classes,'' \emph{IEEE Transactions on Knowledge and Data Engineering}, vol.~28, no.~6, pp. 1532--1545, 2016.

\bibitem{ahmadi2018modeling}
Z.~Ahmadi and S.~Kramer, ``Modeling recurring concepts in data streams: a graph-based framework,'' \emph{Knowledge and Information Systems}, vol.~55, pp. 15--44, 2018.

\bibitem{pratama2015evolving}
M.~Pratama, J.~Lu, and G.~Zhang, ``Evolving type-2 fuzzy classifier,'' \emph{IEEE Transactions on Fuzzy Systems}, vol.~24, no.~3, pp. 574--589, 2015.

\bibitem{domingos2000mining}
P.~Domingos and G.~Hulten, ``Mining high-speed data streams,'' in \emph{Proceedings of the sixth ACM SIGKDD international conference on Knowledge discovery and data mining}, 2000, pp. 71--80.

\bibitem{hulten2001mining}
G.~Hulten, L.~Spencer, and P.~Domingos, ``Mining time-changing data streams,'' in \emph{Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining}, 2001, pp. 97--106.

\bibitem{yang2012incrementally}
H.~Yang and S.~Fong, ``Incrementally optimized decision tree for noisy big data,'' in \emph{Proceedings of the 1st International Workshop on Big Data, Streams and Heterogeneous Source Mining: Algorithms, Systems, Programming Models and Applications}, 2012, pp. 36--44.

\bibitem{yang2015countering}
------, ``Countering the concept-drift problems in big data by an incrementally optimized stream mining model,'' \emph{Journal of Systems and Software}, vol. 102, pp. 158--166, 2015.

\bibitem{rutkowski2012decision}
L.~Rutkowski, L.~Pietruczuk, P.~Duda, and M.~Jaworski, ``Decision trees for mining data streams based on the mcdiarmid's bound,'' \emph{IEEE Transactions on Knowledge and Data Engineering}, vol.~25, no.~6, pp. 1272--1279, 2012.

\bibitem{rutkowski2014new}
L.~Rutkowski, M.~Jaworski, L.~Pietruczuk, and P.~Duda, ``A new method for data stream mining based on the misclassification error,'' \emph{IEEE transactions on neural networks and learning systems}, vol.~26, no.~5, pp. 1048--1059, 2014.

\bibitem{frias2016online}
I.~Frias-Blanco, J.~del Campo-Avila, G.~Ramos-Jimenez, A.~C. Carvalho, A.~Ortiz-D{\'\i}az, and R.~Morales-Bueno, ``Online adaptive decision trees based on concentration inequalities,'' \emph{Knowledge-Based Systems}, vol. 104, pp. 179--194, 2016.

\bibitem{baena2006early}
M.~Baena-Garc{\i}a, J.~del Campo-{\'A}vila, R.~Fidalgo, A.~Bifet, R.~Gavalda, and R.~Morales-Bueno, ``Early drift detection method,'' in \emph{Fourth international workshop on knowledge discovery from data streams}, vol.~6.\hskip 1em plus 0.5em minus 0.4em\relax Citeseer, 2006, pp. 77--86.

\bibitem{madkour2023historical}
A.~H. Madkour, A.~Elsayed, and H.~Abdel-Kader, ``Historical isolated forest for detecting and adaptation concept drifts in nonstationary data streaming,'' \emph{IJCI. International Journal of Computers and Information}, vol.~10, no.~2, pp. 16--27, 2023.

\bibitem{tan2022information}
C.~H. Tan, V.~C. Lee, and M.~Salehi, ``Information resources estimation for accurate distribution-based concept drift detection,'' \emph{Information Processing \& Management}, vol.~59, no.~3, p. 102911, 2022.

\bibitem{adams2023explainable}
J.~N. Adams, S.~J. van Zelst, T.~Rose, and W.~M. van~der Aalst, ``Explainable concept drift in process mining,'' \emph{Information Systems}, vol. 114, p. 102177, 2023.

\bibitem{page1954continuous}
E.~S. Page, ``Continuous inspection schemes,'' \emph{Biometrika}, vol.~41, no. 1/2, pp. 100--115, 1954.

\bibitem{jackowski2014improved}
K.~Jackowski, B.~Krawczyk, and M.~Wo{\'z}niak, ``Improved adaptive splitting and selection: the hybrid training method of a classifier based on a feature space partitioning,'' \emph{International journal of neural systems}, vol.~24, no.~03, p. 1430007, 2014.

\bibitem{yin2022graph}
T.~Yin, C.~Liu, F.~Ding, Z.~Feng, B.~Yuan, and N.~Zhang, ``Graph-based stock correlation and prediction for high-frequency trading systems,'' \emph{Pattern Recognition}, vol. 122, p. 108209, 2022.

\bibitem{ren2023grouping}
J.~Ren, Y.~Wang, Y.-m. Cheung, X.-Z. Gao, and X.~Guo, ``Grouping-based oversampling in kernel space for imbalanced data classification,'' \emph{Pattern Recognition}, vol. 133, p. 108992, 2023.

\bibitem{nitesh2002smote}
V.~C. Nitesh, ``Smote: synthetic minority over-sampling technique,'' \emph{J Artif Intell Res}, vol.~16, no.~1, p. 321, 2002.

\bibitem{han2005borderline}
H.~Han, W.-Y. Wang, and B.-H. Mao, ``Borderline-smote: a new over-sampling method in imbalanced data sets learning,'' in \emph{International conference on intelligent computing}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2005, pp. 878--887.

\bibitem{bunkhumpornpat2009safe}
C.~Bunkhumpornpat, K.~Sinapiromsaran, and C.~Lursinsap, ``Safe-level-smote: Safe-level-synthetic minority over-sampling technique for handling the class imbalanced problem,'' in \emph{Advances in knowledge discovery and data mining: 13th Pacific-Asia conference, PAKDD 2009 Bangkok, Thailand, April 27-30, 2009 proceedings 13}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2009, pp. 475--482.

\bibitem{maciejewski2011local}
T.~Maciejewski and J.~Stefanowski, ``Local neighbourhood extension of smote for mining imbalanced data,'' in \emph{2011 IEEE symposium on computational intelligence and data mining (CIDM)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2011, pp. 104--111.

\bibitem{gao2020saccos}
Y.~Gao, S.~Chandra, Y.~Li, L.~Khan, and T.~Bhavani, ``Saccos: A semi-supervised framework for emerging class detection and concept drift adaption over data streams,'' \emph{IEEE Transactions on Knowledge and Data Engineering}, vol.~34, no.~3, pp. 1416--1426, 2020.

\bibitem{masud2010classification}
M.~Masud, J.~Gao, L.~Khan, J.~Han, and B.~M. Thuraisingham, ``Classification and novel class detection in concept-drifting data streams under time constraints,'' \emph{IEEE Transactions on knowledge and data engineering}, vol.~23, no.~6, pp. 859--874, 2010.

\bibitem{haque2016sand}
A.~Haque, L.~Khan, and M.~Baron, ``Sand: Semi-supervised adaptive novel class detection and classification over data stream,'' in \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, vol.~30, no.~1, 2016.

\bibitem{mu2017streaming}
X.~Mu, F.~Zhu, J.~Du, E.-P. Lim, and Z.-H. Zhou, ``Streaming classification with emerging new class by class matrix sketching,'' in \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, vol.~31, no.~1, 2017.

\bibitem{mu2017classification}
X.~Mu, K.~M. Ting, and Z.-H. Zhou, ``Classification under streaming emerging new classes: A solution using completely-random trees,'' \emph{IEEE Transactions on Knowledge and Data Engineering}, vol.~29, no.~8, pp. 1605--1618, 2017.

\bibitem{zhu2020semi}
Y.-N. Zhu and Y.-F. Li, ``Semi-supervised streaming learning with emerging new labels,'' in \emph{Proceedings of the AAAI Conference on Artificial Intelligence}, vol.~34, no.~04, 2020, pp. 7015--7022.

\bibitem{cai2019nearest}
X.-Q. Cai, P.~Zhao, K.-M. Ting, X.~Mu, and Y.~Jiang, ``Nearest neighbor ensembles: An effective method for difficult problems in streaming classification with emerging new classes,'' in \emph{2019 IEEE international conference on data mining (ICDM)}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2019, pp. 970--975.

\bibitem{zhang2022knnens}
J.~Zhang, T.~Wang, W.~W. Ng, and W.~Pedrycz, ``Knnens: A k-nearest neighbor ensemble-based method for incremental learning under data stream with emerging new classes,'' \emph{IEEE transactions on neural networks and learning systems}, vol.~34, no.~11, pp. 9520--9527, 2022.

\bibitem{long2013transfer}
M.~Long, J.~Wang, G.~Ding, J.~Sun, and P.~S. Yu, ``Transfer feature learning with joint distribution adaptation,'' in \emph{Proceedings of the IEEE international conference on computer vision}, 2013, pp. 2200--2207.

\bibitem{long2014transfer}
------, ``Transfer joint matching for unsupervised domain adaptation,'' in \emph{Proceedings of the IEEE conference on computer vision and pattern recognition}, 2014, pp. 1410--1417.

\bibitem{sun2011two}
Q.~Sun, R.~Chattopadhyay, S.~Panchanathan, and J.~Ye, ``A two-stage weighting framework for multi-source domain adaptation,'' \emph{Advances in neural information processing systems}, vol.~24, 2011.

\bibitem{freund1996experiments}
Y.~Freund, R.~E. Schapire \emph{et~al.}, ``Experiments with a new boosting algorithm,'' in \emph{icml}, vol.~96.\hskip 1em plus 0.5em minus 0.4em\relax Citeseer, 1996, pp. 148--156.

\bibitem{sun2016return}
B.~Sun, J.~Feng, and K.~Saenko, ``Return of frustratingly easy domain adaptation,'' in \emph{Proceedings of the AAAI conference on artificial intelligence}, vol.~30, no.~1, 2016.

\bibitem{rahman2020correlation}
M.~M. Rahman, C.~Fookes, M.~Baktashmotlagh, and S.~Sridharan, ``Correlation-aware adversarial domain adaptation and generalization,'' \emph{Pattern Recognition}, vol. 100, p. 107124, 2020.

\bibitem{zhong2009cross}
E.~Zhong, W.~Fan, J.~Peng, K.~Zhang, J.~Ren, D.~Turaga, and O.~Verscheure, ``Cross domain distribution adaptation via kernel mapping,'' in \emph{Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining}, 2009, pp. 1027--1036.

\bibitem{powers2020evaluation}
D.~M. Powers, ``Evaluation: from precision, recall and f-measure to roc, informedness, markedness and correlation,'' \emph{arXiv preprint arXiv:2010.16061}, 2020.

\bibitem{liu2022multi}
B.~Liu, K.~Blekas, and G.~Tsoumakas, ``Multi-label sampling based on local label imbalance,'' \emph{Pattern Recognition}, vol. 122, p. 108294, 2022.

\bibitem{lopez2012analysis}
V.~L{\'o}pez, A.~Fern{\'a}ndez, J.~G. Moreno-Torres, and F.~Herrera, ``Analysis of preprocessing vs. cost-sensitive learning for imbalanced classification. open problems on intrinsic data characteristics,'' \emph{Expert Systems with Applications}, vol.~39, no.~7, pp. 6585--6608, 2012.

\bibitem{zhang2020towards}
M.-L. Zhang, Y.-K. Li, H.~Yang, and X.-Y. Liu, ``Towards class-imbalance aware multi-label learning,'' \emph{IEEE Transactions on Cybernetics}, vol.~52, no.~6, pp. 4459--4471, 2020.

\bibitem{liu2017regional}
A.~Liu, Y.~Song, G.~Zhang, and J.~Lu, ``Regional concept drift detection and density synchronized drift adaptation,'' in \emph{IJCAI International Joint Conference on Artificial Intelligence}, 2017.

\bibitem{da2014learning}
Q.~Da, Y.~Yu, and Z.-H. Zhou, ``Learning with augmented class by exploiting unlabeled data,'' in \emph{Proceedings of the AAAI conference on artificial intelligence}, vol.~28, no.~1, 2014.

\bibitem{sasaki2007truth}
Y.~Sasaki \emph{et~al.}, ``The truth of the f-measure. teach tutor mater, 1 (5), 1--5,'' 2007.

\bibitem{brodersen2010balanced}
K.~H. Brodersen, C.~S. Ong, K.~E. Stephan, and J.~M. Buhmann, ``The balanced accuracy and its posterior distribution,'' in \emph{2010 20th international conference on pattern recognition}.\hskip 1em plus 0.5em minus 0.4em\relax IEEE, 2010, pp. 3121--3124.

\bibitem{kubat1997addressing}
M.~Kubat, S.~Matwin \emph{et~al.}, ``Addressing the curse of imbalanced training sets: one-sided selection,'' in \emph{Icml}, vol.~97, no.~1.\hskip 1em plus 0.5em minus 0.4em\relax Citeseer, 1997, p. 179.

\bibitem{krawczyk2017ensemble}
B.~Krawczyk, L.~L. Minku, J.~Gama, J.~Stefanowski, and M.~Wo{\'z}niak, ``Ensemble learning for data stream analysis: A survey,'' \emph{Information Fusion}, vol.~37, pp. 132--156, 2017.

\bibitem{ksieniewicz2022stream}
P.~Ksieniewicz and P.~Zyblewski, ``Stream-learn—open-source python library for difficult data stream batch analysis,'' \emph{Neurocomputing}, vol. 478, pp. 11--21, 2022.

\bibitem{wang2019characterizing}
Z.~Wang, Z.~Dai, B.~P{\'o}czos, and J.~Carbonell, ``Characterizing and avoiding negative transfer,'' in \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, 2019, pp. 11\,293--11\,302.

\bibitem{zadrozny2004learning}
B.~Zadrozny, ``Learning and evaluating classifiers under sample selection bias,'' in \emph{Proceedings of the twenty-first international conference on Machine learning}, 2004, p. 114.

\bibitem{cortes2008sample}
C.~Cortes, M.~Mohri, M.~Riley, and A.~Rostamizadeh, ``Sample selection bias correction theory,'' in \emph{International conference on algorithmic learning theory}.\hskip 1em plus 0.5em minus 0.4em\relax Springer, 2008, pp. 38--53.

\bibitem{pan2010domain}
S.~J. Pan, I.~W. Tsang, J.~T. Kwok, and Q.~Yang, ``Domain adaptation via transfer component analysis,'' \emph{IEEE transactions on neural networks}, vol.~22, no.~2, pp. 199--210, 2010.

\bibitem{li2015learning}
P.~Li, X.~Wu, X.~Hu, and H.~Wang, ``Learning concept-drifting data streams with random ensemble decision trees,'' \emph{Neurocomputing}, vol. 166, pp. 68--83, 2015.

\bibitem{cao2019learning}
Z.~Cao, K.~You, M.~Long, J.~Wang, and Q.~Yang, ``Learning to transfer examples for partial domain adaptation,'' in \emph{Proceedings of the IEEE/CVF conference on computer vision and pattern recognition}, 2019, pp. 2985--2994.

\end{thebibliography}
