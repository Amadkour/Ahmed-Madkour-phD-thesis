
% this file is called up by thesis.tex
% content in this file will be fed into the main document

%: ----------------------- introduction file header -----------------------

\begin{savequote}[50mm]
Everything is theoretically impossible, until it is done.
\qauthor{
Robert A. Heinlein}
% The beginning is the most important part of the work.
% \qauthor{Plato}
\end{savequote}

\chapter{Conclusions and Future Work}
\label{cha:7_Conclusions}
In this chapter, we summarize the main contributions, limitations, and future lines of research resulted from this thesis. First, a consolidated view of results and
contributions is presented in Section \ref{ch7:contributions}. Afterward, the limitations of this work are identified in Section \ref{ch7:limitations}. Finally, the future research lines from this thesis are exposed in Section \ref{ch7:futurework}.



\section{Summary of Contributions} \label{ch7:contributions}
Machine learning is an artificial intelligence technology that gives systems the ability to learn and develop from experience automatically without being programmed specifically. The primary aim is to help computers learn automatically without human intervention or assistance. This field still receiving great interest due to the growth of data, computing power, and statistical algorithms. While data mining is a closely related topic, aims to discover useful, valid, understandable, and unexpected patterns from data. However, each learning algorithm discovers the pattern from a different perspective. Regarding that, ensemble data mining has been proved as an optimal strategy to aggregate and combine several learning algorithms together. For the classification task, the aggregated models are well-known with the name of multiple classifier systems (MCS). The main objective of the thesis is to enhance the generation and the integration of MCS via soft computing techniques. 



In the context discussed above, MCS are hybrid intelligent systems with the potentiality to cope with ambiguity, uncertainty, and complex problems. While soft computing methods support intelligent control, nonlinear programming, optimization, and decision making. Soft computing methods exploit the tolerance for imprecision, partial truth, and uncertainty to achieve tractability, robustness, low cost solution. With the human mind as a role model, soft computing enables solutions for problems that may be either unsolvable or just too time-consuming. This could be particularly helpful to optimize the design and the integration of the complex MCS.

The research developed in this dissertation has contributed to the enhancement of MCS. The experiments have been conducted on popular benchmark datasets. In chapter \ref{cha:2_background}, we presented a nice taxonomy for MCS. Afterward, we have discussed the importance of diversity and how we can measure it through pairwise and non-pairwise metrics. Furthermore, the concept of error diversity is presented to alleviate the consolidated error. Finally, the importance of soft computing is highlighted within the area of MCS. In chapter \ref{cha:3_State-of-the-art}, we have reviewed the state-of-the-art classifier ensembles, this included; bagging-like ensembles, boosting, and gradient boosting ensembles. The presented algorithms were compared together in terms of how to promote diversity, and the type of the base model. Afterward, due to the complexity of MCS, metaheuristic algorithms (MA) were specifically applied to better integrate or prune the classifiers set. Finally, from the conducted revision, we identified the gaps and the research questions that we answered in this thesis. Among the chief contributions of this thesis:

Chapter \ref{chapter4_training-set}, is dedicated to the first objective: \textit{"To build more diverse and highly accurate MCS, only from a reduced portion of the available data"}. The complexity of the classifiers ensemble increases positively with the size of training data. To reduce this complexity, IS techniques could be used as a preliminary step to reduce the training data-size. First, the border noisy samples are removed via intelligent data sampling. Hence, pure, reduced, and informative data samples can be obtained to train individual classifiers quickly and correctly. Second, the proposed MCS, Section \ref{proposed.mcs}, considers two strategies to promote diversity; data manipulation and algorithm manipulation techniques. For data manipulation, the bagging and the random feature selection are applied to the reduced dataset from phase one. While, for algorithmic manipulation, the diversity is promoted via generating heterogeneous classifiers. Finally, SI algorithms were incorporated to enhance MCS predictivity. Where, a weighted voting schema is optimized via MFO, GWO, and WOA algorithms to enhance the fusion of multiple decisions. The novelty of this contribution is the intersection between three computational intelligence paradigms: instance selection, ensemble learning, and swarm intelligence. The effect of IS and SI on the generalization performance of MCS has been analyzed and discussed in detail in Sections \ref{comb.SI}, and \ref{ch4:explorationofres}, respectively. With the conclusion, IS proved its effectiveness to reduce the training data-size of 17 datasets by more than 25\%. AllKNN, as IS technique, did not prove its capability to capture the integrity from the whole data, where RFSM outperform RFCOM in only 4 out of 25 datasets. The mistake of IS to capture informative samples has been compensated via our proposal. The proposed MCS with a simple combination function, majority voting, outperforms RFCOM in 8 out of 25 datasets. While, a great improvement has been achieved via SI, weighted voting strategy, to outperform RFCOM in 14 out of 25 datasets. Concluding, the predetermined first objective is partly achieved due to the limited performance of the IS method. 

Chapter \ref{ch5_GUided_MCS_Pruning}, is dedicated to the second objective: \textit{"Increasing the efficiency of MCS and going beyond what can
be achieved from ensemble pruning methods"}. A framework has been proposed to benefit from the power of instance selection and ensemble selection simultaneously. In relation to that, IS methods can be applied as a kind of data preprocessing to clean and eliminate inconsistent data. While ensemble pruning is the strategy by which a small-size ensemble can be selected without affecting the general performance of the original ensemble. Hence, the computational resources can be saved, and the testing time can be accelerated by depending on some classifiers instead of all. Ensemble pruning is the second component, that was integrated into the framework, Section \ref{ch5_methodology}, to elevate its performance. A guided search-based MCS pruning has been proposed to consider both the classifier’s accuracy and ensemble diversity. First, two ordering-based pruning metrics have been applied, where each of them returns a set of classifiers. The suggested classifier set from each metric is merged together to form a new subensemble to be further searched via metaheuristic methods. The proposal successfully solved the parameter tuning challenges, the alpha parameter in Equation \ref{MDEP}, which is part of a novel and recent pruning metric, MDEP \cite{guo2018}. The limited accuracy of MCS pruning metrics has been maximized via the proposed guided search-based pruning. This has been proved and clarified as in Figure \ref{ch5:comparison.nonreduced}, and according to the statistical analysis, Section \ref{statistical}. In this work, small-size ensembles with training on fewer samples could outperform significantly the large-size ensembles which use the whole available training data. Concluding, the predetermined second objective is totally achieved. 


Chapter \ref{cha:6_our_proposal}, is dedicated to the third objective: \textit{"Grouping and analyzing fast and accurate heuristic metrics for MCS pruning"}. Ensemble pruning strategies are one of the hot topics to gain efficient and effective ensembles. The efficiency could be reached via forming small-size ensembles with their impact; to consume short memory space, reduce the communication cost of the distributed models, and to accelerate the prediction time. While the efficacy could be achieved via building trustable models with a high level of prediction accuracy. We applied a fast and accurate strategy to work with bagging ensembles via ranking the ensemble members. This pruning strategy is known as “ordering-based pruning” to identify the best subset of classifiers for early aggregation. The identification of this subset is mainly controlled through a heuristic measurement to optimize the augmented subensemble. In this chapter, greedy search methods and group-based ranking have been considered to reorder the pool of classifiers. Since the great analysis by Martínez-Muñoz et al. \cite{martinez2009} in 2009, several heuristic metrics belonging to this category were proposed \cite{lu2010,guo2013,guo2018,cao2018} with no existence comparison between them. Regarding that, our proposal was conducted to solve that gap. The conclusions from this analysis proved that the efficacy of those metrics is affected by the original ensemble size, the required subensemble size, the kind of individual classifiers, and the number of classes. Furthermore, the investigated metrics realize robust and stable predictions, this is analyzed via the range of their prediction accuracy around the median as shown in Figure \ref{ch6_consprediction}. Finally, the computational cost of some metrics is linear with the initial pool size $T$, while other metrics have a larger computational cost, which is quadratic in $T$. Concluding, the predetermined third objective is totally achieved.   

\section{Limitations} \label{ch7:limitations}

In relation to the contributions summarised above, this thesis also presents limitations
that need to be mentioned. In this section, we highlight some aspects that are a barrier to better improvement. 

\begin{itemize}
    \item In Chapter \ref{chapter4_training-set}, the proposed MCS uses reduced data for training. However, we cannot guarantee whether or not the reduced data via IS could keep the integrity from the original data. In some cases, the reduced data miss informative samples which leads to bad training. This has been analyzed and clarified via the performance on $D_{14}$, $D_{19}$ from Table \ref{table-accuracy}.
    \item In Chapter \ref{chapter4_training-set}, the proposed approach is characterized by a relatively high computational complexity. Therefore, it is not suitable for \emph{online} learning, e.g., in the case of nonstationary data classification streams, namely, when the \emph{concept drift} phenomenon can occur. Instead, it can be a suitable alternative if the training time is not a critical parameter from the application perspective.
    
    \item In Chapter \ref{ch5_GUided_MCS_Pruning}, the guided search-based pruning identifies a subensemble with higher predictive performance, but the other pruning metrics (EPIC, UMEP, MDEP) identify a thinner/small-size subensemble. This is clarified by the statistical analysis in Table \ref{wilcoxontable}. 
    
    \item The presented work in this thesis has not been scaled up to prove its suitability for big datasets. In addition, the investigated work did not consider problems with certain properties, like imbalanced class labels.  

\end{itemize}


\section{Future works}\label{ch7:futurework}
As the limitations of our study indicate, there are numerous ways to beneficially extend this line of research in the future. They range from improving the training phase of MCS to pruning and combining MCS efficiently.  


\begin{enumerate}
     \item The proposed MCS is heterogeneous, containing different models, with the aim to promote diversity. While the identification of classifiers to be included in classifier ensembles remains a key issue for predictive performance. In \cite{onan2016}, the classifiers types to be consolidated are selected manually via the majority voting error and forward search. Experimentally, the set of classifiers solving a task could not be effective for another task \citep{wolpert2002}. The inclusion of weak classifiers could degrade the general performance unless if they create complement decisions for particular samples, this is too complex and need to be further discovered and analyzed via tailored metrics to determine the individual classifier type. 
    \item In Chapters \ref{chapter4_training-set} and \ref{ch5_GUided_MCS_Pruning}, AllKNN \cite{tomek1976}, as a training set selection, has been applied due to its reasonable selection time, reduction rate, and limited-accuracy over test \cite{garcia2011}. An interesting research line would be to evaluate the performance of other training set selection algorithms. Particularly, how each reduction method could add to the performance of MCS. In addition, the concept of instance selection had been widely used with the support vector machine classifier \cite{liu2017,chen2013} to alleviate its computational difficulties when dealing with huge amounts of data. In connection with that, a Pareto-based SVM ensemble has been proposed in \cite{rosales2017} to optimize the size of the training set and the classification performance attained by the selection of the instances. Thus, the consideration of those modeling approaches could potentially lead to new promising versions for both homogeneous and heterogeneous MCS.      
    
    \item In Chapter \ref{ch5_GUided_MCS_Pruning}, We have identified that both the classifier's accuracy and the ensemble diversity are crucial for MCS pruning. Our proposed schema is so simple and innovative. The computational cost of our solution is light, due to the usage of ordering metrics, in comparison to evolutionary algorithms. While multi-objective optimization could be further applied to tune the calibration between the subensemble size and the subensemble accuracy. Examples of recent works in this area \cite{fletcher2020}.
    
    \item The concept of uncorrelated error, Section \ref{ch2_uncorrelated}, is so important to alleviate the consolidated error. Regarding that, the weights of Equation (\ref{weighted.count.errors}) can be optimized by evolutionary algorithms. Furthermore, the subensemble could be better located if the concept of uncorrelated error can be incorporated in the investigated metrics of Chapter \ref{cha:6_our_proposal}.
    
    \item In Chapter \ref{chapter4_training-set}, the SI-based weighted voting schema considers the abstract predictions from the individual classifiers. In contrast, the class probability distribution carries information that should not be ignored \cite{sikora2015}. For example, if we take the votes according to the class labels, then the incorrect outputs of the mistaken members will be amplified into wrong votes. A future research line could be interesting to apply SI for the class probability distribution. Furthermore, transforming the outputs of ensemble members into class labels before considering their soft votes, leads to destroying the real distribution and losing useful information \cite{dai2015}.  Besides, the usage of soft voting could be more efficient for ensemble pruning via decreasing the probability that a tie occurs, i.e in majority voting. 
    \item In \cite{li2014}, the authors recommended using the classification confidence and the weights of the base classifiers to define the ensemble margin. They used a homogeneous ensemble of SVM, and the classification confidence for each classifier is calculated in terms of the distance between sample $\textbf{x}_i$ and the hyperplane. However, it is not clear how to measure the classification confidence for different classifier types, an interesting research line could be to extend their proposal to adapt to a heterogeneous ensemble. In addition, in the same article, the weighted voting of the pruned ensemble proved superior results, in terms of accuracy, over the majority voting of the pruned ensemble. Regarding that, the investigated metrics of Chapters \ref{ch5_GUided_MCS_Pruning} and \ref{cha:6_our_proposal} could be further enhanced. 
\end{enumerate}
       

By taking these issues into account, the predictive performance of MCS scheme could be further enhanced.