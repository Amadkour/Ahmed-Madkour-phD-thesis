\section{Transfer Learning} 
\label{sec:3_5_transfer_learning}
Transfer learning has gained significant attention for addressing distribution disparities between source and target domains, with methods falling into two categories: instance re-weighting and feature matching \cite{long2013transfer}. Instance re-weighting methods, such as Kernel Mean Matching (KMM) \cite{long2014transfer}, Kullback-Leibler Importance Estimation Procedure (KLIEP) \cite{sun2011two}, and TrAdaBoost \cite{freund1996experiments}, adjust the weights of source instances to align with the target domain. These methods have been extended to multisource transfer learning (e.g., MsTrAdaBoost \cite{sun2016return}). Feature matching approaches, including Transfer Component Analysis (TCA) \cite{sun2016return} and CORAL \cite{rahman2020correlation}, focus on aligning feature representations between domains, often through transformations that minimize distribution differences. However, negative transfer, where transferred knowledge harms performance, is a challenge addressed by techniques like Transfer Joint Matching (TJM) \cite{zhong2009cross}. Additionally, methods such as HE-CDTL \cite{powers2020evaluation} handle concept drift in transfer learning by incorporating historical knowledge and class-wise weighted ensembles. Finally, online transfer learning techniques like Melanie \cite{sun2016return} manage non-stationary environments by dynamically adjusting model weights to accommodate concept drift.
