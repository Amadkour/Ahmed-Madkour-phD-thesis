%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%   General-purpose Automated Machine Learning
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Classifier Ensemble Selection}

\label{sec:3_2_ensemble}
This study examines the overproduce-and-select approach for classifier ensemble selection \cite{cruz2017meta, kuncheva2000clustering, jackowski2014improved}. The goal is to identify the optimal subset of classifiers from a larger ensemble, considering factors like performance, diversity metrics, meta-learning techniques, and performance estimation. This process reduces computational complexity, enhances efficiency, and improves ensemble performance, particularly for real-world applications. By selecting a smaller subset, the approach balances accuracy with computational resources and adapts to evolving data streams. There are two main selection strategies: static and dynamic. Static selection assigns classifiers to specific feature space partitions, while dynamic selection chooses classifiers based on local competencies for each data sample. Dynamic Ensemble Selection (DES) selects the best classifiers for each test instance, considering their competence in the local region. The Randomized Reference Classifier, proposed by Woloszynski and Kurzynski \cite{woloszynski2011probabilistic}, enhances adaptability and robustness by introducing randomness through a beta distribution. This classifier is particularly effective in concept drift scenarios. However, using diversity measures, as shown by Lysiak \cite{lysiak2014optimal}, may lead to smaller ensembles but not necessarily improved accuracy. Overall, the overproduce-and-select approach offers a framework for tackling concept drift by dynamically adapting the ensemble composition, improving classification performance, efficiency, and adaptability in dynamic environments.