%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%   Machine Learning Modelling Approaches of Traffic Forecasting
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Soft Computing}
\label{sec:2_4_soft-computing}
Soft computing is one of the pioneer computing paradigms which resembles the human mind's remarkable capacity to think, understand, and solve difficult real-life problems \cite{das2013,konar2018}. Soft computing exploits the tolerance for imprecision, partial truth, and uncertainty to achieve robustness, tractability, low solution cost, and very high-performance \cite{zhu2015}. Modern machinery is more complex to be controlled and stabilized. The reasons for this difficulty is the lack of numerical models that describe exactly how they work, and the existence of many nonlinear and time-variant plants. Soft computing methods support intelligent control, nonlinear programming, optimization, and decision making support. Among those methods; fuzzy logic, genetic algorithms, artificial neural network. Soft computing has become popular with their wide applications to many research fields as speech recognition, communication, pattern recognition, signal processing, automatic control engineering. In their connectivity with the area of MCS, we present several soft computing methods that have been used to improve efficiency:


 \textbf{Fuzzy Logic:}
In domains and environments that are realistic, incomplete evidence inevitably emerges. During experiments, noise corruption or instrument errors may give rise to data when a certain value is measured, leading to incomplete data. In other scenarios, collecting the correct information can be excessively costly or unviable. In addition, using extra information from an expert, which is usually given by fuzzy logic and fuzzy sets, can be useful. Typically, data has a certain degree of vagueness. If the imprecision is significant, then the imprecise values must be handled in all the phases of learning and classification. To address the uncertainty of decision trees with minor disruptions, fuzzy sets and their underlying approximate reasoning capacities have been paired with decision trees in \cite{yuan1995,olaru2003}. The resulting trees exhibit increased tolerance to noise, and extend applicability to ambiguous or unclear circumstances. In \cite{bonissone2010}, a fuzzy random forest was suggested to increase the diversity of the trees through randomness, with the versatility of handling incomplete data. The numeric attributes are discretized through a fuzzy partition, so each internal node in the fuzzy decision tree constructs a child node for each fuzzy set of the partition. Then the membership degree of the examples to different fuzzy sets is determined to optimize the node-split attribute. In addition, a fuzzy combination method for one-class classifiers has been proposed in \cite{wilk2012}.             

\textbf{Evolutionary Algorithms:} Evolutionary algorithms uses mechanisms inspired by biological evolution, such as reproduction, mutation, recombination, and selection to solve complex problems. For example, genetic algorithm (GA) \cite{Holland1992} emulates the natural and human evolution, where common genes from parents can be transferred to their children. GA evolves an initial population of chromosomes; each chromosome represents one solution in the search space. The algorithm depends on applying crossover and mutation operators to explore and exploit different regions from the search space, which probably contains promising solutions. The process continues for a specific number of generations or till reaching threshold error or if the algorithm is trapped in a local minimum without the ability to find more accurate solutions. Ensemble systems are very complex architectures that need to be optimized. GA has been applied in \cite{ruta2005} to control the ensemble size (classifier subset selection) via optimizing several criterias. While in \cite{sikora2015} GA has been considered as a trainable combiner to tune the classifiers' weights for the fusion process. Furthermore,  Kuncheva et al. \cite{kuncheva2000d} applied two versions of GA for selecting feature subsets to be used by base models. While tsymbal et al. \cite{tsymbal2005} applied GA to optimize the diversity of the best-collected feature subsets.            

\textbf{Artificial Neural Networks:}
Neural networks were proved to be universal approximators with unlimited flexibility.  In any number of dimensions, they could approximate any classification boundary. However, this capability comes at a price. There is a need to train large systems with a huge number of parameters. Then, an acceptable architecture with the tuned parameters will be trusted for all future classifications. In the face of several local minima, all global optimization methods yield "optimal" parameters ($w$) that differ significantly from one run of the algorithm to the next. This reveals a great deal of randomness arising from various initial weights ($w^0$) and various sequencing of training examples. This randomness appears to distinguish between network errors. The final weights correspond to various ways of identifying the training pattern. Hence, the collective decision created by several ANNs may, therefore, be much less fallible than any network. In \cite{kim2010}, the ANNs in the form of base classifiers were generated to form bagging and boosting ensembles to predict the bankruptcy. While in \cite{plesinger2018} a convolutional neural network is linked to a shallow neural network to classify arrhythmia in a Holter ECG signal.


\textbf{Swarm Intelligence:}  SI algorithms are defined as \textit{"nature-inspired algorithms that concern the collective, emerging behavior of multiple, interacting agents that follow some simple rules"} \cite{hassanien2018}. These algorithms mimic the social behavior of swarms/groups of creatures in nature.  In \cite{mirjalili2014}, the authors discussed the benefits of SI over evolutionary algorithms.  Most SI methods consider exploration and exploitation in their working mechanisms \cite{mirjalili2010}. The popularity of those algorithms returns to; the simplicity of inspiration, flexibility, derivative-free mechanism, and local optimum avoidance \cite{mirjalili2014}. Firefly algorithm has been applied in \cite{krawczyk2015} to combine the ensemble pruning with a weighted classifier fusion module. In addition, the Ant colony algorithm has been incorporated to optimize the decision forest \cite{kozak2015} to provide self-adaptability with the classification task. Recent SI algorithms will be discussed in Sections \ref{Sec:4_3_5-MFO}, \ref{GWOalgo}, and \ref{Sec:4_3_5-WOA}.







