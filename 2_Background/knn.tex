\section{K-Nearest Neighbors (KNN) Algorithm}
\label{sec:knn}
The K-Nearest Neighbors \cite{zhang2022knnens} (KNN) algorithm is a simple, yet effective, method used in machine learning for classification and regression tasks. It operates on the principle of identifying the "K" closest data points to a given query point in the feature space and making predictions based on the majority class (for classification) or the average value (for regression) of those neighbors. The main advantages of KNN are its ease of implementation and ability to handle both linear and non-linear decision boundaries without requiring an explicit model to be built. However, KNN has some limitations, particularly when it comes to large datasets and high-dimensional data, as the computation of distances can become expensive.

In the context of streaming data, where new data points are constantly being added, KNN is often applied to predict the class of an incoming data point by comparing it to the most recent observations in the stream. However, one of the challenges when using KNN in dynamic environments is dealing with concept drift. Since KNN relies on the most recent data points to make predictions, its performance can degrade significantly when the statistical properties of the stream change over time, as is the case with concept drift.