%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%   Methodology
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experimental Results}
\label{sec:6_2_methodology}

The experiments are dedicated to achieve objective 3, to group and analyze fast and accurate heuristic metrics for MCS pruning. The five main questions to be answered are:

\begin{itemize}[nosep]
\setlength{\itemindent}{-.5in}
    \item $\pmb{Q_5}$. How the initial classifier pool size and the required subensemble size affect the performance of heuristic pruning metrics?
    \item  $\pmb{Q_6}$. How the heuristic pruning metrics are affected by the individual classifier type? 
    \item $\pmb{Q_7}$. How the efficacy of the pruning metrics could be affected by binary and multiclass datasets?   
     \item $\pmb{Q_8}$. How the pruning metrics are effective to reduce the performance variance?
     \item $\pmb{Q_9}$. How the efficiency of the heuristic pruning metrics differs, in terms of time and space complexities?
\end{itemize}


\subsection{Set Up} \label{ch6_setup}
The design of experiments has considered the recommendations from \cite{martinez2009} according to the following two issues: (A) The influence of training conditions (B) The influence of the initial pool size. \textit{For training conditions}; the whole training data has been used, for both, to train the bagged ensemble and to prune it. \textit{For the initial pool size}; the initial pool should contain a sufficient number of classifiers. However, the gained accuracy is not worthing the added complexity resulting from expanding the pool. In this chapter, two ensemble systems have been constructed as a part of the analysis. 
 
\begin{enumerate}[nosep]
    \item Heterogeneous  (\textit{Different Classifier types-DC})
       \begin{itemize}[nosep]
           \item[-] \textit{Samples}: Bootstrap samples, with replacement, are generated from the training data.
           \item[-] \textit{Features}: Sixty percent of features are selected randomly for each classifier.
           \item[-] \textit{Classifiers}: Five different classifier models, with their default setup parameters, have been used  ( \textit{DT}\footnote{Package C50 :https://cran.r-project.org/web/packages/C50\label{Decisiont}}, \textit{NB}\footnote{Package e1071:https://cran.r-project.org/web/packages/e1071}, \textit{JRip}\footnote{Package RWeka:https://cran.r-project.org/web/packages/RWeka}, \textit{Multinom}\footnote{Package nnet:http://cran.r-project.org/web/packages/nnet} and \textit{KNN}\footnote{Package caret:https://cran.r-project.org/web/packages/caret}) with 20\% as a proportional representation by each model  from the whole pool size.
       \end{itemize}
    \item Homogeneous  (\textit{Similar Classifiers-SC})
     \begin{itemize}[nosep]
         \item[-] Similar to the previous, while the difference is that all individual members are of type \textit{DT}\textsuperscript{\ref{Decisiont}}. 
     \end{itemize}
\end{enumerate} 




Finally, all the datasets are preprocessed by unifying the scales of the features via normalization. For each dataset, 10 repetitions of 10 fold cross-validation procedure have been tested to get 100 runs per dataset. In addition, MDEP depends on an internal parameter $\alpha$; three values for MDEP with different $\alpha \in \{0.1, 0.5,0.9\}$ are considered, and the best-optimized alpha according to the in train-validation is used to report the test for each dataset separately. The results for Random Forest (RF) \cite{breiman2001}, Adaboost (AdaB) \cite{freund1997}, and the single best model (SBM)  from the pool, \textit{according to the measured accuracy of the models on the pruning set}, are included as references in the comparison.     

The default set-up for the individual classifiers types, RF, and AdaB are as follows: DT uses C5.0 decision trees of Quinlan \cite{quinlan2014} in its pruned version. NB uses Naïve Bayes with Laplace smoothing to solve the problem of zero probability. KNN applies $k$-nearest neighbor classification with $k=3$ as the number of neighbors. RF\footnote{randomForest:https://cran.r-project.org/web/packages/randomForest} implements Breiman’s random forest algorithm with ntrees=$T$, number of variables that are randomly sampled at each split=$sqrt(ncol(\textbf{x}))$. AdaB\footnote{Adaboost:https://cran.r-project.org/web/packages/adabag} fits the AdaBoost.M1 using classification trees as base classifiers with iterations=$T$.


A total of 30 datasets that were obtained from OpenML\footnote{ Machine Learning Repository: https://www.openml.org} and KEEL \footnote{KEEL Repository: http://www.keel.es} are used in this study for experimentation. The characteristics of the data are presented in Table \ref{ch6_table_data}, where \#S, \#F, \#C, and R represent the number of samples, the number of features, the number of classes, and the ratio between the smallest, and the largest class for each dataset respectively. The number of classes varies from 2 to 10, while the maximum number of features is 100. 



\vspace*{.3cm}

\begin{table*}[!ht]
\centering\scriptsize
\caption{Characteristics of the selected datasets for experimentation, \textit{sorted by samples and classes}.}
 \label{ch6_table_data}
\resizebox{\textwidth}{!}{

\begin{tabular}{l|cccc||l|cccc}
  \hline
 DataSet & \#S & \#F &  \#C & R  &DataSet & \#S & \#F &  \#C & R \\ 
  \hline
  Breast-cancer & 286 & 9 & 2 & 0.42 & Wine &178 &13 &3 &0.676 \\
  SPECTF & 349 & 44 & 2 & 0.37 & Newthyroid  &215 & 5& 3& 0.2\\  
  Ionosphere & 351 & 33 & 2 & 0.56 & Cmc  &\numprint{1473} &9 &3 &0.529 \\ 
  Wdbc & 569 & 30 & 2 &  0.594 & Lymphography &148 &18 &4 &0.025 \\ 
  Indian Liver Patient (ILP) & 583 & 10 & 2 &  0.401 & Vehicle &846 &18 &4 &0.913 \\ 
  Australian & 690 & 14 &  2 & 0.802 & Wall-Following-Robot (WFR) & \numprint{5456}& 24&4 &0.149 \\
  Wisconsin & 699 & 9 &  2 & 0.526 & Cleveland &297 &13 &5 &0.081 \\
  Blood-transfusion & 748 & 4 & 2 & 0.312  & Dermatology &358 &34 &6 &0.18 \\
  Mammographic & 830 & 5 & 2 & 0.944  &  Flare &\numprint{1066} &11 &6 &0.13 \\ 
  Tic-tac-toe & 958 & 9 & 2 & 0.530  & Wine quality-red &\numprint{1599} &11 &6 &0.015 \\ 
 German & \numprint{1000} & 20 & 2 & 0.429 & Satimage & \numprint{6435} &36 &6 &0.408 \\  
 Hill-valley & \numprint{1212} & 100 & 2 & 1.0   & Segment  & \numprint{2310}&18 &7 &1 \\ 
 Kr-vs-kp &\numprint{3196} & 36 & 2 & 0.915 & Led7digit  &500 &7 &10 &0.649 \\
 spambase &\numprint{4601} & 57 & 2 & 0.650 & Mfeat-Karh &\numprint{2000} &64 &10 &1 \\   
 Ringnorm & \numprint{7400} & 20 & 2 & 0.981   & Mfeat-Fourier &\numprint{2000} &76 &10 &1 \\   
 
\hline
\end{tabular}
}
\label{dataset}
\end{table*}