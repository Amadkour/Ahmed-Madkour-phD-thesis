\section{Proposed Methodology}\label{sec:4_first_proposed_approach}

In this section, we introduce the Heterogeneous Transfer Learning (HTL) algorithm tailored for non-stationary environments. HTL adeptly assimilates knowledge from both heterogeneous and homogeneous sources, operating within the framework of data streams subject to concept drift. Leveraging an online learning inductive parameter transfer strategy, HTL achieves seamless knowledge transfer. We delineate the core components and workflow of the HTL algorithm. To address the challenges intrinsic to our approach, we focus on three key aspects.
•	
•	
•	

\begin{itemize}
	\item \textbf{Heterogeneous Sources:} Traditional transfer learning methods often assume homogeneity in the data distribution across source and target domains. However, in real-world scenarios, data sources may vary significantly in terms of feature space. HTL addresses this challenge by allowing knowledge transfer from sources with different dimensionalities. This means that the algorithm can effectively learn from diverse dimentionality sources, enabling a more comprehensive knowledge transfer process.
	\item \textbf{Drifted Streams:} In dynamic environments, data distributions may change over time, leading to concept drift. This phenomenon poses a significant challenge for machine learning algorithms, as models trained on historical data may become obsolete as the underlying data distribution shifts. HTL incorporates a concept drift detector that continuously monitors the incoming data stream. Upon detecting a shift in the data distribution, the algorithm adapts its classifiers accordingly to ensure continued effectiveness in handling drifting streams. This dynamic adjustment mechanism allows HTL to maintain high performance even in the presence of concept drift.
	\item \textbf{Classifier Performance:} Classifier performance is crucial for the overall effectiveness of the transfer learning process. HTL employs Dynamic Ensemble Selection (DES) to enhance classifier performance. DES creates a diverse ensemble of classifiers, each trained on different subsets of the data. When presented with a new data point, DES dynamically selects the most suitable classifiers from the ensemble based on its performance on similar chunk. This adaptive selection process ensures that the most appropriate classifier is chosen for each data point, leading to improved classification accuracy and robustness. By leveraging DES, HTL maximizes the utility of available classifiers, resulting in superior performance in non-stationary environments with heterogeneous data sources.
\end{itemize}

\subsection{HTL Overall Details}

The aim of this section is to harness knowledge from diverse dimensional multi-source domain streams. Following a similar framework to CDTL [43], this approach employs a class-wise and domain-weighted strategy. However, HTL enhances the weight function of CDTL in a class-wise manner, as demonstrated in Equation 1. This equation factors in both correct and incorrect predictions for each classifier class, with K representing the number of classifiers in the current chunk, C denoting the number of classes in the current chunk, and i indicating the current chunk. The components of HTL operate synergistically, as depicted in Fig. 1, encompassing three phases:
\begin{itemize}
	\item \textbf{Dynamic Ensemble Selection phase (DES Phase):}: The primary objective DES phase is to identify the optimal classifier for incoming data. This is crucial for ensuring that the selected classifier effectively aligns with the unique characteristics of the current data segment.
	\item \textbf{Drift detector phase:} After the DES phase, our method advances to the drift detector phase, where it operates in real-time to continually monitor the data stream. This phase employs ADWIN and DDM techniques, which play a pivotal role in swiftly identifying any signs of concept drift. These techniques are designed to detect changes in the underlying data distribution over time, thus enabling the algorithm to adapt to evolving data patterns.
	\item \textbf{•	Feature Scaling} The concluding phase of our approach is featuring scaling, operating in real-time to harmonize the diverse dimensionalities of the multi-source streams with the target dimensionality. Leveraging the eigen vector technique, this phase facilitates the transformation of data into a unified dimensionality, essential for creating new classifiers tailored to the target and domain streams. By ensuring compatibility with the learning framework, this process enhances the algorithm's effectiveness in handling diverse dimensionality streams.
\end{itemize}
\subsection{Classifier Creation Details }

Fig. 2 provides a comprehensive overview of the classifier creation phase, a crucial component responsible for generating new classifiers based on the current chunk of the target domain, previous classifiers, and source domain classifiers. This phase offers several advantages and perform three tasks. \begin{itemize}
	\item \textbf{•	Source Projection:} This step involves projecting the source domain classifiers onto the current chunk of the target domain. The projection likely employs the source weight function, as described in CDTL [43], to assign a weight to each classifier based on its relevance to the current chunk of data. This weighting mechanism ensures that classifiers from the source domains contribute appropriately to the creation of the new classifier for the target domain.
	\item \textbf{Class-Wise Weights:} This block computes class-specific weights for each classifier using Equations 1 and 2. The process involves analyzing the current chunk of the target stream (denoted as \emph{D}) and considering both correct and incorrect predictions made by each classifier. These weights are crucial for determining the significance of individual classifiers across different classes. The equations facilitate the calculation of weights that reflect the classifiers' performance on specific classes, enabling the creation of a well-balanced ensemble classifier.
	\item \textbf{Classifier Combination:} In this phase, the class-wise weights, along with the current chunk of data and the projected source domain classifiers, are combined to derive the final classifier. The combination process follows the details outlined in Eq. 3, where historical classifiers (denoted as \emph{H}), projected data classifiers (denoted as \emph{P}), and the classifier of the current chunk (denoted as \emph{K}) are integrated. This integration ensures that the resulting classifier incorporates contributions from both historical and projected data sources, leveraging the strengths of each to enhance predictive performance. The resulting classifier is then utilized to make predictions based on the data chunk, effectively leveraging the insights gleaned from both historical and current data sources.
\end{itemize}

\subsection{HTL Detailed Algorithm}

As illustrated in Algorithm 1, the HTL algorithm involves several stages, beginning with training a classifier for the target stream and proceeding through various steps related to heterogeneous multisource preprocessing, classifier weighting, concept drift detection, and classifier management to ensure the accuracy and adaptability of the final classifier. In this section, detailed explanations of each individual step within the HTL algorithm are provided.
\begin{itemize}
	\item \textbf{Converting heterogeneous multisource to homogeneous multisource:} In the initial step, the algorithm unifies the various data sources that might have different characteristics (heterogeneous multisource). This is achieved using eigenvectors and the feature count of the current chunk (line 7).
	\item \textbf{Training a new classifier for the first target chunk:} In line 8, the new classifier is trained specifically for the first chunk of the target stream. This classifier was used to predict the initial chunk of the target stream.
	\item \textbf{Calculate heterogeneous multi-source weights:} The algorithm computes the weights for each heterogeneous data source. These weights are determined based on the characteristics of the data from each source and the target classifier. This weighting process helps prioritize more relevant data sources (line 9).
	\item \textbf{Calculating class-wise weights:} In line 27, the algorithm calculates class-wise weights using Equations 1 and 2. These weights were essential for determining the contribution of each class to the final classifier.
	\item \textbf{Converting homogeneous multisource to projected data source:} Line 11 involves using the weights calculated in the third step to transform the homogeneous multisource representation to a projected data source. This transformation likely uses the source weight function of CDTL [43].
	\item \textbf{Prediction of the current chunk:} In line 16, the algorithm determines the output class using Equation 3.
	\item \textbf{Monitoring for concept drift:} The algorithm continuously monitors the accuracy of its predictions to detect any concept drift, a situation in which the underlying data distribution changes, potentially leading to a degradation in model performance. Line 19 has been used for this purpose.
	\item \textbf{Updating the Projected Multi-source classifiers:}Lines 29–32 are dedicated to updating the classifiers associated with the projected multi-source. It is necessary to adapt to changes in the data or to maintain the accuracy and relevance of the classifiers over time.
	\item \textbf{Managing the pool of classifiers:} If the pool of classifiers exceeds a predefined maximum size threshold, lines 23 and 24 indicate that a new classifier is trained, and the worst-performing classifier is removed. This process helps to maintain a manageable and effective set of classifiers.
\end{itemize}
The heterogeneous Transfer Learning (HTL) algorithm represents a significant advancement in addressing the complexities of nonstationary environments prone to concept drift. By integrating heterogeneous and homogeneous sources within dynamic data streams, HTL demonstrates remarkable adaptability and effectiveness. Through its meticulously designed workflow, HTL can efficiently handle the challenges posed by evolving data landscapes. From the initial reception of environmental data to the nuanced processing of new data chunks and vigilant monitoring of concept drift, HTL embodies a comprehensive approach to knowledge transfer and adaptation. Owing to its ability to compute class-specific weights and leverage vital classifiers, HTL offers a robust solution for navigating nonstationary environments with confidence and precision.


\begin{figure}[!ht]
	\centering
	\includegraphics[width=1\linewidth]{6_transfer_learning\figures\alg1.png}
	\caption{Proposed Approch Flow.}
	\label{fig:6_alg1}
\end{figure}
\begin{figure}[!ht]
	\centering
	\includegraphics[width=1\linewidth]{6_transfer_learning\figures\alg2.png}
	\caption{Classifier Creation Approach}
	\label{fig:6_alg2}
\end{figure}

\begin{equation}
	\label{eq:4_first_proposal_1}
    frq_{c} = \sum_{i=1}^{\text{chunk size}} \begin{cases} 
    1, & \text{if } y_i = c \\
    0, & \text{otherwise}
    \end{cases}, \quad i = 1, 2, 3, \dots \text{chunk size}\;
\end{equation}

\begin{equation}
	\label{eq:4_first_proposal_2}
	classifierWeight = \sum_{k=1}^{K} \sum_{c=1}^{C} classWeight_{k,c,D} \quad D = 1, 2, 3, \dots, N
\end{equation}

\begin{equation}
	\label{eq:4_first_proposal_3}
    \text{classes type}_{\text{chunk}} = \sum_{c=1}^{C} \begin{cases} 
    \text{Minority,} & \text{if } diff(sd_c - frq_i) > \text{best } freq_{\text{chunk}} \\
    \text{Majority,} & \text{otherwise}
    \end{cases}, \quad c = 1, 2, 3, \dots C
\end{equation}


\begin{algorithm}[H]
\caption{Proposed Framework Algorithm for Imbalanced Multi-Class Drifted Data Streams}
\KwIn{data stream, maximum classifiers pool size $\kappa$}
% \Parameter{current chunk $a$, synthetic data $b$, classifiers pool $\Psi$, drifted pool $\psi$, classes frequency $\Omega$, best frequency $\omega$, minority classes $\mu$}
\KwOut{Prediction $P$}
\BlankLine
$\psi, \Psi, \Omega, \mu \gets \emptyset$\;
$\omega \gets 0$\;
\For{stream have chunk}{
    \eIf{$a$ is the First chunk}{
        $k \gets$ \texttt{trainingNewClassifier}($a$)\;
        $P \gets$ \texttt{getPrediction}($a, k$)\;
    }{
        $k \gets$ \texttt{DES}($a, \Psi$)\;
        $P \gets$ \texttt{getPrediction}($a, k$)\;
        $\psi \gets$ \texttt{conceptDriftDetector}($P$)\;
        \If{$\psi > 0$}{
            $\Omega \gets$ get classes frequency according to Eq.1\;
            $\omega \gets$ best frequency according to Eq.2\;
            $\mu \gets$ get minority classes according to Eq.3\;
            $b \gets$ utilize $a$ and $\mu$ to get the synthetic data according to Algorithm 2\;
            trainingData $\gets a + b$\;
            $k \gets$ \texttt{trainingNewClassifier}(trainingData)\;
            $\Psi \gets \Psi + k$\;
            \If{$\Psi > \kappa$}{
                \texttt{removeWorstClassifier}($\Omega$)\;
            }
        }
        $P \gets$ \texttt{getPrediction}($a, k$)\;
    }
}
\Return{$P$}
\end{algorithm}

\vspace{1cm}

\begin{algorithm}[H]
	\caption{Synthetic data generator}
	\label{alg:4_first_proposal_2}
	\KwIn{Minority classes $\mu$, current chunk $a$, sample size $\eta$, historical chunks $h$}
	\KwOut{Generated data $b$}
	$b \gets \emptyset$\;
	$f \gets \text{MLSMSOTE}$\;
	$knn \gets \text{kNearestNeighbor}(a)$\;
	$chunk \gets \text{similarChunk}(a, h)$\;
	$f \gets \text{similarChunkOverSamplingMethod}(chunk)$\;
	\If{$f = \text{MLSMSOTE}$}{
		$f \gets \text{MLSOL}$\;
	}
	\Else{
		$f \gets \text{MLSMSOTE}$\;
	}
	\While{$|b| < \eta$}{
		$p \gets \text{generateSyntheticPoint}(\mu, f)$\;
		$similarPointsClass \gets \text{KNN.getKneighbor}(b)$\;
		\If{$similarPointsClass = \mu$}{
			$b \gets b \cup \{p\}$\;
		}
	}
	\Return $b$\;
	\end{algorithm}