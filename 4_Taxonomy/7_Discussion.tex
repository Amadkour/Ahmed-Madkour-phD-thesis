
\section{Discussion}
 \label{sec:4_7_Discussion}
The diversity of the proposed MCS is maximized via three steps (bagging, Hamming distance-based feature selection, and heterogeneous classifiers). Sixty percent of the features are selected randomly for each classifier according to the maximum Hamming distance over the ensemble. This percentage can be changed or tuned, or intelligent feature selection can be considered for the construction of the MCS.   

The effect of scaling up the data to be handled by this framework has not been studied in this article. However, this is possible via the application of the stratification mechanism \cite{garcia2011}, via which a huge train can be stratified into several parts while preserving the class distribution in each part. Then, the reduced data will be formed by combining the outputs of the application of IS to each part individually. 


The proposed method can be successfully used in predictive system learning, especially if the problem is characterized by high dimensionality due to both the number of attributes and the number of records. 

The time complexity of the proposed framework can be divided into two parts: The first part is the \textit{overhead time complexity}, which is proportional to the properties of the IS method. For that, AllKNN has been selected due to its reasonable selection time. The second part is the \textit{ensemble time complexity}, where the MCS training time will be reduced due to the size of the selected data. In addition, the weight tuning time for the aggregation of classifier decisions by SI can be controlled by tuning the algorithmic parameters; the population size, and the number of iterations.

However, similar to popular approaches that are based on deep learning (DL), the proposed approach is characterized by a relatively high computational complexity. Therefore, it is not suitable for \emph{online} learning, e.g., in the case of non-stationary data classification streams, namely, when the \emph{concept drift} phenomenon can occur. Instead, it can be a suitable alternative if the training time is not a critical parameter from the application perspective.


Nevertheless, the proposed framework can outperform DL methods according to the following: MCS has less tendency to overfit, especially in this proposed framework, because heterogeneous classifiers are trained from bootstrapped samples over various sub-features. DL is based on a complex model that fits a dataset well. Additionally, MCS can be more easily deployed in production systems than DL. Moreover, DL requires a large amount of data for success and its training is complicated as no precise method is available for obtaining the best set of hyper-parameters \cite{litjens2017}.
